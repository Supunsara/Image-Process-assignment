{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "# Normalize pixel values\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "std=1e-5\n",
    "H= 200\n",
    "w1 = std*np.random.randn(Din,H)\n",
    "w2 = std*np.random.randn(H,K)\n",
    "b1 = np.zeros(H)\n",
    "b2 = np.zeros(K)\n",
    "batch_size = 500\n",
    "\n",
    "iterations = 300\n",
    "lr = 1.4e-2\n",
    "lr_decay=0.999\n",
    "reg = 5e-6\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration 0 / 300: loss 0.797939\n",
      "iteration 1 / 300: loss 0.772448\n",
      "iteration 2 / 300: loss 0.758266\n",
      "iteration 3 / 300: loss 0.767201\n",
      "iteration 4 / 300: loss 0.742996\n",
      "iteration 5 / 300: loss 0.742108\n",
      "iteration 6 / 300: loss 0.711456\n",
      "iteration 7 / 300: loss 0.700853\n",
      "iteration 8 / 300: loss 0.709203\n",
      "iteration 9 / 300: loss 0.682689\n",
      "iteration 10 / 300: loss 0.692339\n",
      "iteration 11 / 300: loss 0.672449\n",
      "iteration 12 / 300: loss 0.679613\n",
      "iteration 13 / 300: loss 0.676706\n",
      "iteration 14 / 300: loss 0.652313\n",
      "iteration 15 / 300: loss 0.649647\n",
      "iteration 16 / 300: loss 0.660421\n",
      "iteration 17 / 300: loss 0.629696\n",
      "iteration 18 / 300: loss 0.642286\n",
      "iteration 19 / 300: loss 0.655187\n",
      "iteration 20 / 300: loss 0.624543\n",
      "iteration 21 / 300: loss 0.620080\n",
      "iteration 22 / 300: loss 0.609595\n",
      "iteration 23 / 300: loss 0.619506\n",
      "iteration 24 / 300: loss 0.641974\n",
      "iteration 25 / 300: loss 0.594697\n",
      "iteration 26 / 300: loss 0.608114\n",
      "iteration 27 / 300: loss 0.588154\n",
      "iteration 28 / 300: loss 0.581116\n",
      "iteration 29 / 300: loss 0.583420\n",
      "iteration 30 / 300: loss 0.587386\n",
      "iteration 31 / 300: loss 0.608824\n",
      "iteration 32 / 300: loss 0.592388\n",
      "iteration 33 / 300: loss 0.611995\n",
      "iteration 34 / 300: loss 0.630292\n",
      "iteration 35 / 300: loss 0.605423\n",
      "iteration 36 / 300: loss 0.593047\n",
      "iteration 37 / 300: loss 0.578442\n",
      "iteration 38 / 300: loss 0.606669\n",
      "iteration 39 / 300: loss 0.605112\n",
      "iteration 40 / 300: loss 0.616545\n",
      "iteration 41 / 300: loss 0.581201\n",
      "iteration 42 / 300: loss 0.596803\n",
      "iteration 43 / 300: loss 0.583583\n",
      "iteration 44 / 300: loss 0.595801\n",
      "iteration 45 / 300: loss 0.607713\n",
      "iteration 46 / 300: loss 0.603727\n",
      "iteration 47 / 300: loss 0.575475\n",
      "iteration 48 / 300: loss 0.586925\n",
      "iteration 49 / 300: loss 0.591666\n",
      "iteration 50 / 300: loss 0.604783\n",
      "iteration 51 / 300: loss 0.615748\n",
      "iteration 52 / 300: loss 0.564277\n",
      "iteration 53 / 300: loss 0.573195\n",
      "iteration 54 / 300: loss 0.588663\n",
      "iteration 55 / 300: loss 0.598071\n",
      "iteration 56 / 300: loss 0.599841\n",
      "iteration 57 / 300: loss 0.572514\n",
      "iteration 58 / 300: loss 0.571295\n",
      "iteration 59 / 300: loss 0.553255\n",
      "iteration 60 / 300: loss 0.605471\n",
      "iteration 61 / 300: loss 0.586970\n",
      "iteration 62 / 300: loss 0.603870\n",
      "iteration 63 / 300: loss 0.576328\n",
      "iteration 64 / 300: loss 0.597446\n",
      "iteration 65 / 300: loss 0.596755\n",
      "iteration 66 / 300: loss 0.602356\n",
      "iteration 67 / 300: loss 0.566298\n",
      "iteration 68 / 300: loss 0.581782\n",
      "iteration 69 / 300: loss 0.587494\n",
      "iteration 70 / 300: loss 0.599334\n",
      "iteration 71 / 300: loss 0.570501\n",
      "iteration 72 / 300: loss 0.588039\n",
      "iteration 73 / 300: loss 0.587541\n",
      "iteration 74 / 300: loss 0.577437\n",
      "iteration 75 / 300: loss 0.593234\n",
      "iteration 76 / 300: loss 0.578474\n",
      "iteration 77 / 300: loss 0.571156\n",
      "iteration 78 / 300: loss 0.594745\n",
      "iteration 79 / 300: loss 0.571852\n",
      "iteration 80 / 300: loss 0.594889\n",
      "iteration 81 / 300: loss 0.586149\n",
      "iteration 82 / 300: loss 0.596445\n",
      "iteration 83 / 300: loss 0.596726\n",
      "iteration 84 / 300: loss 0.618731\n",
      "iteration 85 / 300: loss 0.597007\n",
      "iteration 86 / 300: loss 0.588614\n",
      "iteration 87 / 300: loss 0.601856\n",
      "iteration 88 / 300: loss 0.593975\n",
      "iteration 89 / 300: loss 0.573104\n",
      "iteration 90 / 300: loss 0.594542\n",
      "iteration 91 / 300: loss 0.576902\n",
      "iteration 92 / 300: loss 0.589583\n",
      "iteration 93 / 300: loss 0.576213\n",
      "iteration 94 / 300: loss 0.605935\n",
      "iteration 95 / 300: loss 0.583448\n",
      "iteration 96 / 300: loss 0.562458\n",
      "iteration 97 / 300: loss 0.575084\n",
      "iteration 98 / 300: loss 0.574856\n",
      "iteration 99 / 300: loss 0.575895\n",
      "iteration 100 / 300: loss 0.586992\n",
      "iteration 101 / 300: loss 0.572562\n",
      "iteration 102 / 300: loss 0.628759\n",
      "iteration 103 / 300: loss 0.573035\n",
      "iteration 104 / 300: loss 0.586075\n",
      "iteration 105 / 300: loss 0.575856\n",
      "iteration 106 / 300: loss 0.574615\n",
      "iteration 107 / 300: loss 0.573807\n",
      "iteration 108 / 300: loss 0.587468\n",
      "iteration 109 / 300: loss 0.610233\n",
      "iteration 110 / 300: loss 0.594247\n",
      "iteration 111 / 300: loss 0.592290\n",
      "iteration 112 / 300: loss 0.601501\n",
      "iteration 113 / 300: loss 0.587678\n",
      "iteration 114 / 300: loss 0.569865\n",
      "iteration 115 / 300: loss 0.585326\n",
      "iteration 116 / 300: loss 0.580582\n",
      "iteration 117 / 300: loss 0.602264\n",
      "iteration 118 / 300: loss 0.606553\n",
      "iteration 119 / 300: loss 0.558697\n",
      "iteration 120 / 300: loss 0.595652\n",
      "iteration 121 / 300: loss 0.588305\n",
      "iteration 122 / 300: loss 0.591663\n",
      "iteration 123 / 300: loss 0.570514\n",
      "iteration 124 / 300: loss 0.569369\n",
      "iteration 125 / 300: loss 0.554229\n",
      "iteration 126 / 300: loss 0.582577\n",
      "iteration 127 / 300: loss 0.575110\n",
      "iteration 128 / 300: loss 0.596784\n",
      "iteration 129 / 300: loss 0.580092\n",
      "iteration 130 / 300: loss 0.588805\n",
      "iteration 131 / 300: loss 0.584066\n",
      "iteration 132 / 300: loss 0.569807\n",
      "iteration 133 / 300: loss 0.575540\n",
      "iteration 134 / 300: loss 0.602148\n",
      "iteration 135 / 300: loss 0.587525\n",
      "iteration 136 / 300: loss 0.581001\n",
      "iteration 137 / 300: loss 0.595003\n",
      "iteration 138 / 300: loss 0.593413\n",
      "iteration 139 / 300: loss 0.596766\n",
      "iteration 140 / 300: loss 0.619988\n",
      "iteration 141 / 300: loss 0.571521\n",
      "iteration 142 / 300: loss 0.576170\n",
      "iteration 143 / 300: loss 0.598800\n",
      "iteration 144 / 300: loss 0.594541\n",
      "iteration 145 / 300: loss 0.599357\n",
      "iteration 146 / 300: loss 0.585896\n",
      "iteration 147 / 300: loss 0.583043\n",
      "iteration 148 / 300: loss 0.592397\n",
      "iteration 149 / 300: loss 0.585125\n",
      "iteration 150 / 300: loss 0.584637\n",
      "iteration 151 / 300: loss 0.579207\n",
      "iteration 152 / 300: loss 0.592861\n",
      "iteration 153 / 300: loss 0.569970\n",
      "iteration 154 / 300: loss 0.606675\n",
      "iteration 155 / 300: loss 0.587538\n",
      "iteration 156 / 300: loss 0.587841\n",
      "iteration 157 / 300: loss 0.601092\n",
      "iteration 158 / 300: loss 0.607014\n",
      "iteration 159 / 300: loss 0.580254\n",
      "iteration 160 / 300: loss 0.588637\n",
      "iteration 161 / 300: loss 0.586160\n",
      "iteration 162 / 300: loss 0.593506\n",
      "iteration 163 / 300: loss 0.588528\n",
      "iteration 164 / 300: loss 0.564335\n",
      "iteration 165 / 300: loss 0.604305\n",
      "iteration 166 / 300: loss 0.589751\n",
      "iteration 167 / 300: loss 0.585723\n",
      "iteration 168 / 300: loss 0.572451\n",
      "iteration 169 / 300: loss 0.572687\n",
      "iteration 170 / 300: loss 0.589211\n",
      "iteration 171 / 300: loss 0.605944\n",
      "iteration 172 / 300: loss 0.591841\n",
      "iteration 173 / 300: loss 0.601408\n",
      "iteration 174 / 300: loss 0.583156\n",
      "iteration 175 / 300: loss 0.600504\n",
      "iteration 176 / 300: loss 0.584032\n",
      "iteration 177 / 300: loss 0.583315\n",
      "iteration 178 / 300: loss 0.577079\n",
      "iteration 179 / 300: loss 0.608535\n",
      "iteration 180 / 300: loss 0.583796\n",
      "iteration 181 / 300: loss 0.590214\n",
      "iteration 182 / 300: loss 0.584373\n",
      "iteration 183 / 300: loss 0.600893\n",
      "iteration 184 / 300: loss 0.566338\n",
      "iteration 185 / 300: loss 0.567072\n",
      "iteration 186 / 300: loss 0.595677\n",
      "iteration 187 / 300: loss 0.579118\n",
      "iteration 188 / 300: loss 0.591890\n",
      "iteration 189 / 300: loss 0.585561\n",
      "iteration 190 / 300: loss 0.597933\n",
      "iteration 191 / 300: loss 0.579228\n",
      "iteration 192 / 300: loss 0.572268\n",
      "iteration 193 / 300: loss 0.603726\n",
      "iteration 194 / 300: loss 0.567400\n",
      "iteration 195 / 300: loss 0.580762\n",
      "iteration 196 / 300: loss 0.581008\n",
      "iteration 197 / 300: loss 0.582214\n",
      "iteration 198 / 300: loss 0.598102\n",
      "iteration 199 / 300: loss 0.585599\n",
      "iteration 200 / 300: loss 0.594139\n",
      "iteration 201 / 300: loss 0.566193\n",
      "iteration 202 / 300: loss 0.573627\n",
      "iteration 203 / 300: loss 0.584157\n",
      "iteration 204 / 300: loss 0.580667\n",
      "iteration 205 / 300: loss 0.586747\n",
      "iteration 206 / 300: loss 0.593140\n",
      "iteration 207 / 300: loss 0.577220\n",
      "iteration 208 / 300: loss 0.596714\n",
      "iteration 209 / 300: loss 0.594594\n",
      "iteration 210 / 300: loss 0.613122\n",
      "iteration 211 / 300: loss 0.594292\n",
      "iteration 212 / 300: loss 0.580299\n",
      "iteration 213 / 300: loss 0.590292\n",
      "iteration 214 / 300: loss 0.576066\n",
      "iteration 215 / 300: loss 0.593111\n",
      "iteration 216 / 300: loss 0.588795\n",
      "iteration 217 / 300: loss 0.585070\n",
      "iteration 218 / 300: loss 0.583668\n",
      "iteration 219 / 300: loss 0.575842\n",
      "iteration 220 / 300: loss 0.591575\n",
      "iteration 221 / 300: loss 0.586789\n",
      "iteration 222 / 300: loss 0.585051\n",
      "iteration 223 / 300: loss 0.586377\n",
      "iteration 224 / 300: loss 0.603226\n",
      "iteration 225 / 300: loss 0.565910\n",
      "iteration 226 / 300: loss 0.583717\n",
      "iteration 227 / 300: loss 0.550851\n",
      "iteration 228 / 300: loss 0.578364\n",
      "iteration 229 / 300: loss 0.574725\n",
      "iteration 230 / 300: loss 0.581309\n",
      "iteration 231 / 300: loss 0.562330\n",
      "iteration 232 / 300: loss 0.590332\n",
      "iteration 233 / 300: loss 0.590116\n",
      "iteration 234 / 300: loss 0.580913\n",
      "iteration 235 / 300: loss 0.592273\n",
      "iteration 236 / 300: loss 0.595531\n",
      "iteration 237 / 300: loss 0.571408\n",
      "iteration 238 / 300: loss 0.579259\n",
      "iteration 239 / 300: loss 0.590526\n",
      "iteration 240 / 300: loss 0.568289\n",
      "iteration 241 / 300: loss 0.604764\n",
      "iteration 242 / 300: loss 0.582143\n",
      "iteration 243 / 300: loss 0.604301\n",
      "iteration 244 / 300: loss 0.578662\n",
      "iteration 245 / 300: loss 0.573649\n",
      "iteration 246 / 300: loss 0.564862\n",
      "iteration 247 / 300: loss 0.601160\n",
      "iteration 248 / 300: loss 0.588376\n",
      "iteration 249 / 300: loss 0.586888\n",
      "iteration 250 / 300: loss 0.588322\n",
      "iteration 251 / 300: loss 0.603812\n",
      "iteration 252 / 300: loss 0.588821\n",
      "iteration 253 / 300: loss 0.581121\n",
      "iteration 254 / 300: loss 0.573145\n",
      "iteration 255 / 300: loss 0.596055\n",
      "iteration 256 / 300: loss 0.593233\n",
      "iteration 257 / 300: loss 0.597606\n",
      "iteration 258 / 300: loss 0.589611\n",
      "iteration 259 / 300: loss 0.573431\n",
      "iteration 260 / 300: loss 0.600764\n",
      "iteration 261 / 300: loss 0.588840\n",
      "iteration 262 / 300: loss 0.578031\n",
      "iteration 263 / 300: loss 0.597729\n",
      "iteration 264 / 300: loss 0.594176\n",
      "iteration 265 / 300: loss 0.594794\n",
      "iteration 266 / 300: loss 0.586013\n",
      "iteration 267 / 300: loss 0.595420\n",
      "iteration 268 / 300: loss 0.601061\n",
      "iteration 269 / 300: loss 0.575590\n",
      "iteration 270 / 300: loss 0.584258\n",
      "iteration 271 / 300: loss 0.573234\n",
      "iteration 272 / 300: loss 0.596737\n",
      "iteration 273 / 300: loss 0.603199\n",
      "iteration 274 / 300: loss 0.564215\n",
      "iteration 275 / 300: loss 0.578849\n",
      "iteration 276 / 300: loss 0.588850\n",
      "iteration 277 / 300: loss 0.594864\n",
      "iteration 278 / 300: loss 0.613828\n",
      "iteration 279 / 300: loss 0.590866\n",
      "iteration 280 / 300: loss 0.576710\n",
      "iteration 281 / 300: loss 0.585261\n",
      "iteration 282 / 300: loss 0.580914\n",
      "iteration 283 / 300: loss 0.580378\n",
      "iteration 284 / 300: loss 0.577752\n",
      "iteration 285 / 300: loss 0.583347\n",
      "iteration 286 / 300: loss 0.586137\n",
      "iteration 287 / 300: loss 0.563707\n",
      "iteration 288 / 300: loss 0.577567\n",
      "iteration 289 / 300: loss 0.590409\n",
      "iteration 290 / 300: loss 0.590897\n",
      "iteration 291 / 300: loss 0.589926\n",
      "iteration 292 / 300: loss 0.587131\n",
      "iteration 293 / 300: loss 0.601352\n",
      "iteration 294 / 300: loss 0.581226\n",
      "iteration 295 / 300: loss 0.589312\n",
      "iteration 296 / 300: loss 0.589631\n",
      "iteration 297 / 300: loss 0.605176\n",
      "iteration 298 / 300: loss 0.573978\n",
      "iteration 299 / 300: loss 0.591112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    x = x_train[indices]\n",
    "    y = y_train[indices]\n",
    "\n",
    "    n = int(Ntr/batch_size)\n",
    "    x_batches = np.array_split(x,n)\n",
    "    y_batches = np.array_split(y,n)\n",
    "\n",
    "    for i in range(n):\n",
    "        h = 1.0/(1.0+np.exp(-(x_batches[i].dot(w1)+b1)))\n",
    "        y_pred = h.dot(w2)+b2\n",
    "        loss = 1./batch_size*np.square(y_pred-y_batches[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "        loss_history.append(loss)\n",
    "        dy_pred = 1./batch_size*2.0*(y_pred-y_batches[i]) #partial derivative of L w.r.t. y_hat\n",
    "        dw2 = h.T.dot(dy_pred)+reg*w2\n",
    "        db2 = dy_pred.sum(axis=0)\n",
    "        dh = dy_pred.dot(w2.T)\n",
    "        dw1 = x_batches[i].T.dot(dh*h*(1-h))+reg*w1\n",
    "        db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "        w1 -= lr*dw1\n",
    "        w2 -= lr*dw2\n",
    "        b1 -= lr*db1\n",
    "        b2 -= lr*db2\n",
    "        lr *= lr_decay\n",
    "    print('iteration %d / %d: loss %f' %(t,iterations,loss))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_acc =  0.9971155555555555\ntrain_loss =  0.9999584551096153\ntest_acc =  0.9971666666666666\ntest_loss =  0.19999227248175722\n"
     ]
    }
   ],
   "source": [
    "indices1 = np.arange(Ntr)\n",
    "rng.shuffle(indices1)\n",
    "\n",
    "indices2 = np.arange(Nte)\n",
    "rng.shuffle(indices2)\n",
    "\n",
    "x_tr = x_train[indices1]\n",
    "y_tr = y_train[indices1]\n",
    "\n",
    "x_te = x_test[indices2]\n",
    "y_te = y_test[indices2]\n",
    "\n",
    "n = int(Ntr/batch_size)\n",
    "x_batches1 = np.array_split(x_tr,n)\n",
    "y_batches1 = np.array_split(y_tr,n)\n",
    "\n",
    "x_batches2 = np.array_split(x_te,n)\n",
    "y_batches2 = np.array_split(y_te,n)\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    #train accuracy,train loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches1[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    train_acc = 1.0 -1/(9*Ntr)*(np.abs(np.argmax(y_batches1[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    train_loss = 1./batch_size*np.square(y_pred-y_batches1[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "    #test accuracy, test loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches2[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    test_acc = 1.0 - 1/(9*Nte)*(np.abs(np.argmax(y_batches2[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    test_loss = 1./batch_size*np.square(y_pred-y_batches2[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "print(\"train_acc = \", train_acc)\n",
    "print(\"train_loss = \", train_loss)\n",
    "print(\"test_acc = \", test_acc)\n",
    "print(\"test_loss = \", test_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}